{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'discharge_with_social_final.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = '''Below this prompt is a patient note. Does the note contain any evidence of homelessness? If it does contain evidence of homelessness return ABACRACADABRA \n",
    "along with direct evidence from the note, otherwise, return NONE with a justification of why there is no evidence of homelessness:\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add your folder path to sys.path\n",
    "folder_path = '../Vornoi/QA/'\n",
    "sys.path.append(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qa_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-medium', use_fast = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_prompt(text, prompt, tokenizer, num_tokens):\n",
    "    prompt_tokens = len(tokenizer.encode(prompt))\n",
    "    length = num_tokens - prompt_tokens\n",
    "    broken_text = []\n",
    "    if len(tokenizer.encode(text)) <= length:\n",
    "        broken_text.append(text)\n",
    "        return broken_text\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    tokenized_text= tokenizer.encode(text)\n",
    "    while len(tokenized_text[i*length: -1]) > length:\n",
    "        broken_text.append(tokenizer.decode(tokenized_text[i*length: (i+1)*length]))\n",
    "        i += 1\n",
    "    \n",
    "    broken_text.append(tokenizer.decode(tokenized_text[i*length: -1]))\n",
    "\n",
    "    broken_text[0] = broken_text[0][5:-1]\n",
    "    return broken_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_homeless = pd.DataFrame(columns=[\"Note\", \"Prompt\", \"Model\", \"Topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 197, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 91, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 87, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 75, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 86, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 83, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 83, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 101, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 260, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 72, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 65, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 75, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 80, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 132, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 107, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 80, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 67, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 78, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at HiTZ/A2T_RoBERTa_SMFA_TACRED-re and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 69, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/user/SDOH/pipelinetest.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvaldi/home/user/SDOH/pipelinetest.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m truncated_text \u001b[39min\u001b[39;00m truncated_sentences:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvaldi/home/user/SDOH/pipelinetest.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m   sequence_to_classify \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOTE \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m prompt_1 \u001b[39m+\u001b[39m truncated_text\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvaldi/home/user/SDOH/pipelinetest.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m   classifier \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mHiTZ/A2T_RoBERTa_SMFA_TACRED-re\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvaldi/home/user/SDOH/pipelinetest.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m                       device_map\u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvaldi/home/user/SDOH/pipelinetest.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m   output \u001b[39m=\u001b[39m classifier(sequence_to_classify)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvaldi/home/user/SDOH/pipelinetest.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdone\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/pipelines/__init__.py:870\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m--> 870\u001b[0m     framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    871\u001b[0m         model,\n\u001b[1;32m    872\u001b[0m         model_classes\u001b[39m=\u001b[39;49mmodel_classes,\n\u001b[1;32m    873\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    874\u001b[0m         framework\u001b[39m=\u001b[39;49mframework,\n\u001b[1;32m    875\u001b[0m         task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m    876\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    877\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    878\u001b[0m     )\n\u001b[1;32m    880\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    881\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/pipelines/base.py:269\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    264\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to load the model with Tensorflow.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m     )\n\u001b[1;32m    268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    271\u001b[0m         model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/modeling_utils.py:3396\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3391\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m   3392\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis model has some weights that should be kept in higher precision, you need to upgrade \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3393\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`accelerate` to properly deal with them (`pip install --upgrade accelerate`).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3394\u001b[0m     )\n\u001b[1;32m   3395\u001b[0m \u001b[39mif\u001b[39;00m device_map \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msequential\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 3396\u001b[0m     max_memory \u001b[39m=\u001b[39m get_balanced_memory(\n\u001b[1;32m   3397\u001b[0m         model,\n\u001b[1;32m   3398\u001b[0m         dtype\u001b[39m=\u001b[39;49mtarget_dtype,\n\u001b[1;32m   3399\u001b[0m         low_zero\u001b[39m=\u001b[39;49m(device_map \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mbalanced_low_0\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   3400\u001b[0m         max_memory\u001b[39m=\u001b[39;49mmax_memory,\n\u001b[1;32m   3401\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdevice_map_kwargs,\n\u001b[1;32m   3402\u001b[0m     )\n\u001b[1;32m   3403\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3404\u001b[0m     max_memory \u001b[39m=\u001b[39m get_max_memory(max_memory)\n",
      "File \u001b[0;32m~/mambaforge/envs/tensorml/lib/python3.11/site-packages/accelerate/utils/modeling.py:838\u001b[0m, in \u001b[0;36mget_balanced_memory\u001b[0;34m(model, max_memory, no_split_module_classes, dtype, special_dtypes, low_zero)\u001b[0m\n\u001b[1;32m    835\u001b[0m     buffer \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    837\u001b[0m \u001b[39m# Compute mean of final modules. In the first dict of module sizes, leaves are the parameters\u001b[39;00m\n\u001b[0;32m--> 838\u001b[0m leaves \u001b[39m=\u001b[39m [n \u001b[39mfor\u001b[39;49;00m n \u001b[39min\u001b[39;49;00m module_sizes \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m([p \u001b[39mfor\u001b[39;49;00m p \u001b[39min\u001b[39;49;00m module_sizes \u001b[39mif\u001b[39;49;00m n \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mor\u001b[39;49;00m p\u001b[39m.\u001b[39;49mstartswith(n \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)]) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m]\n\u001b[1;32m    839\u001b[0m module_sizes \u001b[39m=\u001b[39m {n: v \u001b[39mfor\u001b[39;00m n, v \u001b[39min\u001b[39;00m module_sizes\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m n \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m leaves}\n\u001b[1;32m    840\u001b[0m \u001b[39m# Once removed, leaves are the final modules.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/tensorml/lib/python3.11/site-packages/accelerate/utils/modeling.py:838\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    835\u001b[0m     buffer \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    837\u001b[0m \u001b[39m# Compute mean of final modules. In the first dict of module sizes, leaves are the parameters\u001b[39;00m\n\u001b[0;32m--> 838\u001b[0m leaves \u001b[39m=\u001b[39m [n \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m module_sizes \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m([p \u001b[39mfor\u001b[39;49;00m p \u001b[39min\u001b[39;49;00m module_sizes \u001b[39mif\u001b[39;49;00m n \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mor\u001b[39;49;00m p\u001b[39m.\u001b[39;49mstartswith(n \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)]) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[1;32m    839\u001b[0m module_sizes \u001b[39m=\u001b[39m {n: v \u001b[39mfor\u001b[39;00m n, v \u001b[39min\u001b[39;00m module_sizes\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m n \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m leaves}\n\u001b[1;32m    840\u001b[0m \u001b[39m# Once removed, leaves are the final modules.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/tensorml/lib/python3.11/site-packages/accelerate/utils/modeling.py:838\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    835\u001b[0m     buffer \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    837\u001b[0m \u001b[39m# Compute mean of final modules. In the first dict of module sizes, leaves are the parameters\u001b[39;00m\n\u001b[0;32m--> 838\u001b[0m leaves \u001b[39m=\u001b[39m [n \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m module_sizes \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m([p \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m module_sizes \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m p\u001b[39m.\u001b[39;49mstartswith(n \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)]) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[1;32m    839\u001b[0m module_sizes \u001b[39m=\u001b[39m {n: v \u001b[39mfor\u001b[39;00m n, v \u001b[39min\u001b[39;00m module_sizes\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m n \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m leaves}\n\u001b[1;32m    840\u001b[0m \u001b[39m# Once removed, leaves are the final modules.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "  print(i)\n",
    "  entry = df.loc[i]['text']\n",
    "  sentences = sent_tokenize(entry)\n",
    "  for sentence in sentences:\n",
    "    truncated_sentences = truncate_prompt(sentence, prompt_1, tokenizer, 508)\n",
    "    for truncated_text in truncated_sentences:\n",
    "      sequence_to_classify = f\"NOTE {i} \" + prompt_1 + truncated_text\n",
    "      classifier = pipeline(\"text-generation\", model= \"HiTZ/A2T_RoBERTa_SMFA_TACRED-re\",\n",
    "                          device_map= \"auto\")\n",
    "      output = classifier(sequence_to_classify)\n",
    "      print(\"done\")\n",
    "      data_to_add = {\"Note\": f\"Note {i}\", \"Prompt\": sequence_to_classify, \"Topic\": \"Homelessness\", \"Model\": \"HiTZ/A2T_RoBERTa_SMFA_TACRED-re\"}\n",
    "      df_homeless = pd.concat([df_homeless, pd.DataFrame(data_to_add, index = [0])], ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
