{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add your folder path to sys.path\n",
    "folder_path = '../Vornoi/QA/'\n",
    "sys.path.append(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qa_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f70b2bd4f5f41dbba3f09b77cd1554f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37d6079950c4b95b4d29712bbf52422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "saved_net = BertRegressor.load_from_checkpoint(\"/home/user/SDOH/epoch=14-step=5625.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_router(input):\n",
    "    #Router\n",
    "    #saved net is a bert model. tokenize input and run it through the model\n",
    "    input_ids = tokenizer.encode(input, return_tensors='pt').to(saved_net.device)\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(input_ids.device)\n",
    "    output = saved_net(input_ids, attention_mask)\n",
    "    print(output)\n",
    "    best_model = model_names[output.argmax()]\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f9c767354e4b2b864c9a6cde7c22ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/286 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ba5a9857e647bb89b6221d22353052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-medium', use_fast = False)\n",
    "model_names = [\n",
    "    #XXS (1b order)\n",
    "    # 'microsoft/phi-1_5',\n",
    "    # 'Fredithefish/Guanaco-3B-Uncensored-v2',\n",
    "    # 'EleutherAI/pythia-1b',\n",
    "    # 'PY007/TinyLlama-1.1B-step-50K-105b',\n",
    "    # 'cerebras/btlm-3b-8k-base',\n",
    "    #XS (5b order)\n",
    "    # 'TheBloke/Llama-2-7B-Chat-GGML', #some random error\n",
    "    'TheBloke/Llama-2-7b-Chat-GPTQ', #1s per sentence\n",
    "    # 'TheBloke/Airoboros-L2-7B-2.2-GPTQ', #some random error, ignoring for now\n",
    "    'HyperbeeAI/Tulpar-7b-v0',\n",
    "    # 'Open-Orca/Mistral-7B-OpenOrca',\n",
    "    # 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "    # 'mistralai/Mistral-7B-v0.1',\n",
    "    # 'circulus/Llama-2-7b-orca-v1',\n",
    "    # 'tiiuae/falcon-7b-instruct', #takes too long\n",
    "    # 'meta-llama/Llama-2-7b-hf',\n",
    "    # \"stabilityai/StableBeluga-7B\",\n",
    "    # 'Lajonbot/tableBeluga-7B-instruct-pl-lora_unload',\n",
    "    # 'THUDM/chatglm2-6b',\n",
    "    'lmsys/vicuna-7b-v1.5',\n",
    "    # 'lmsys/vicuna-7b-v1.3',\n",
    "    # 'lmsys/vicuna-7b-v1.1',\n",
    "    # 'TheBloke/Zarablend-L2-7B-GPTQ',\n",
    "    #Small (10b order)\n",
    "    'TheBloke/Spicyboros-13B-2.2-GPTQ',\n",
    "    # 'TheBloke/openchat_v3.2_super-GPTQ', #also slow\n",
    "    'TheBloke/Airoboros-L2-13B-2.2-GPTQ',\n",
    "    # 'TheBloke/Pygmalion-2-13B-GPTQ', #Takes 7s per sentence\n",
    "    # 'PygmalionAI/mythalion-13b',\n",
    "    # 'lmsys/vicuna-13b-v1.5',\n",
    "    # 'lmsys/vicuna-13b-v1.3',\n",
    "    # 'lmsys/vicuna-13b-v1.1',\n",
    "    # 'meta-llama/Llama-2-13b-hf',\n",
    "    # 'AIDC-ai-business/Luban-13B',\n",
    "    # 'uukuguy/speechless-llama2-luban-orca-platypus-13b',\n",
    "    # 'yeontaek/llama-2-13B-ensemble-v5',\n",
    "    # 'TFLai/OpenOrca-Platypus2-13B-QLoRA-0.80-epoch',\n",
    "    # 'garage-bAInd/Stable-Platypus2-13B',\n",
    "    # 'TheBloke/COTHuginn-4.5-19B-GPTQ', # 30 seconds per iteration\n",
    "    'TheBloke/Unholy-v1-10l-13B-GPTQ', #1s per iteration\n",
    "    'TheBloke/Nous-Hermes-13B-Code-GPTQ', #2s per iteration\n",
    "    #Medium (30b order)\n",
    "    # 'garage-bAInd/GPlatty-30B',\n",
    "    # 'Writer/palmyra-20b-chat',\n",
    "    # 'upstage/llama-30b-instruct-2048',\n",
    "    # 'lmsys/vicuna-33b-v1.3',\n",
    "    # 'tiiuae/falcon-40b',\n",
    "    # 'garage-bAInd/SuperPlatty-30B',\n",
    "    # 'CalderaAI/30B-Lazarus',\n",
    "    'TheBloke/30B-Epsilon-GPTQ',\n",
    "    # 'TheBloke/Airoboros-33B-2.1-GPTQ', #some random error\n",
    "    #Large (70b order)\n",
    "    # 'meta-llama/Llama-2-70b-chat-hf',\n",
    "    # 'NousResearch/Nous-Hermes-Llama2-70b',\n",
    "    # 'garage-bAInd/Platypus2-70B-instruct',\n",
    "    # 'fangloveskari/Platypus_QLoRA_LLaMA_70b',\n",
    "    # 'upstage/SOLAR-0-70b-16bit',\n",
    "    # 'chargoddard/MelangeA-70b',\n",
    "    'TheBloke/Airoboros-65B-GPT4-m2.0-GPTQ',\n",
    "    'TheBloke/Llama-2-70B-Ensemble-v5-GPTQ', #3.5 seconds per iteration\n",
    "    'TheBloke/Uni-TianYan-70B-GPTQ', #3s per iteration\n",
    "    # # 'TheBloke/Synthia-70B-v1.2-GPTQ', #3s per example\n",
    "    'TheBloke/ORCA_LLaMA_70B_QLoRA-GPTQ', #3s per example\n",
    "    #XXL (150b order)\n",
    "    # 'TheBloke/Falcon-180B-Chat-GPTQ', # 60s per sample\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9858, 0.7499, 0.9585, 0.7836, 0.8507, 0.8484, 0.8536, 0.7936, 0.6699,\n",
      "         0.9375, 0.9661, 0.9637, 0.9725]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "name = run_router(\"The patient is homeless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TheBloke/Llama-2-7b-Chat-GPTQ'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
